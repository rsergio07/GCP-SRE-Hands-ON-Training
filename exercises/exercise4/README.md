# Exercise 4: Cloud Monitoring Stack

## Table of Contents
- [Introduction](#introduction)
- [Learning Objectives](#learning-objectives)
- [Prerequisites](#prerequisites)
- [Theory Foundation](#theory-foundation)
- [Understanding Your Application's Current Metrics](#understanding-your-applications-current-metrics)
- [Deploying Prometheus for Production Monitoring](#deploying-prometheus-for-production-monitoring)
- [Configuring Google Cloud Monitoring Integration](#configuring-google-cloud-monitoring-integration)
- [Real-Time Monitoring with Multiple Terminals](#real-time-monitoring-with-multiple-terminals)
- [Understanding Prometheus Service Discovery Issues](#understanding-prometheus-service-discovery-issues)
- [Creating and Testing SRE Queries](#creating-and-testing-sre-queries)
- [Final Objective](#final-objective)
- [Troubleshooting Common Issues](#troubleshooting-common-issues)
- [Next Steps](#next-steps)

---

## Introduction

In this exercise, you will implement a comprehensive monitoring stack for your Kubernetes-deployed SRE application. You'll learn how to deploy Prometheus with proper RBAC permissions, handle real-world configuration challenges, and create monitoring that provides actionable insights for SRE decision-making.

This exercise demonstrates the complexities of production monitoring systems and teaches you to diagnose and resolve common observability challenges that SRE teams encounter daily.

---

## Learning Objectives

By completing this exercise, you will understand:

- **Production Monitoring Deployment**: How to deploy Prometheus in Kubernetes with proper RBAC and service discovery
- **Kubernetes RBAC for Monitoring**: How to configure ServiceAccounts, ClusterRoles, and permissions for observability
- **Troubleshooting Service Discovery**: How to diagnose and fix common Prometheus configuration issues
- **Google Cloud Integration**: How to work with Google Cloud Monitoring APIs and handle permission limitations
- **Real-Time Observation**: How to generate traffic and observe metrics propagation in real-time
- **SRE Query Development**: How to create and test queries that support SLI/SLO frameworks
- **Error Handling in Monitoring**: How to handle expected failures and configuration limitations

---

## Prerequisites

Before starting this exercise, ensure you have completed:

- Exercise 1: Cloud Development Environment & SRE Application
- Exercise 2: Container Builds & GitHub Actions  
- Exercise 3: Kubernetes Deployment

**Verify your prerequisites:**

```bash
# Check that your SRE application is running
kubectl get deployment sre-demo-app
kubectl get service sre-demo-service
```

**Expected output:**
```
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
sre-demo-app   2/2     2            2           1h

NAME               TYPE           CLUSTER-IP      EXTERNAL-IP       PORT(S)        AGE
sre-demo-service   LoadBalancer   34.118.234.68   104.154.201.227   80:30123/TCP   1h
```

---

## Theory Foundation

### Observability in Production Systems

**Essential Watching** (15 minutes):
- [Observability vs Monitoring Explained](https://www.youtube.com/watch?v=CAQ_a2-9UOI) by IBM Technology
- [SLIs, SLOs, SLAs, oh my! (class SRE implements DevOps)](https://www.youtube.com/watch?v=tEylFyxbDLE) by Google SRE

**Reference Documentation**:
- [Google SRE Book - Monitoring Distributed Systems](https://sre.google/sre-book/monitoring-distributed-systems/)
- [Prometheus Best Practices](https://prometheus.io/docs/practices/naming/)
- [Kubernetes RBAC Documentation](https://kubernetes.io/docs/reference/access-authn-authz/rbac/)

### Key Concepts for This Exercise

**Service Discovery Challenges**: In real production environments, Prometheus service discovery requires careful configuration of RBAC permissions, proper pod annotations, and network connectivity. You'll experience and resolve these challenges.

**Kubernetes RBAC**: Role-Based Access Control is essential for securing cluster operations. Prometheus requires specific permissions to discover and scrape pods across namespaces. Understanding RBAC is critical for production deployments.

**Metric Propagation Delays**: Understanding why metrics don't appear immediately and how to verify the monitoring pipeline is critical for SRE work. You'll learn to distinguish between configuration issues and normal delays.

**Integration Limitations**: Not all monitoring features work in every environment. You'll learn to work with permission constraints and find alternative approaches when ideal solutions aren't available.

---

## Understanding Your Application's Current Metrics

### Step 1: Navigate to Exercise Environment

Set up your working directory and verify your application's current state:

```bash
# Navigate to Exercise 4 directory
cd exercises/exercise4
```

### Step 2: Examine Your Application's Rich Metrics

Your SRE application has been accumulating valuable metric data since deployment. Let's examine what's available:

```bash
# Get your application's external IP
export EXTERNAL_IP=$(kubectl get service sre-demo-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
echo "Application URL: http://$EXTERNAL_IP"
```

**Expected output:**
```
Application URL: http://104.154.201.227
```

**Examine the metrics your application is currently exposing:**

```bash
# Look at current metrics endpoint
curl -s http://$EXTERNAL_IP/metrics | head -20
```

**Expected output (with rich existing data):**
```
# HELP http_requests_total Total number of HTTP requests
# TYPE http_requests_total counter
http_requests_total{endpoint="ready_check",method="GET",status="200"} 1554.0
http_requests_total{endpoint="index",method="GET",status="200"} 66.0
http_requests_total{endpoint="unknown",method="GET",status="404"} 48.0
http_requests_total{endpoint="get_stores",method="GET",status="200"} 171.0
http_requests_total{endpoint="metrics",method="GET",status="200"} 172.0
# HELP http_requests_created Total number of HTTP requests
# TYPE http_requests_created gauge
http_requests_created{endpoint="ready_check",method="GET",status="200"} 1.757176733358903e+09
```

**Understanding your application's metric richness:**
- **ready_check: 1554 requests**: Kubernetes health checks running consistently
- **index: 66 requests**: User traffic to your application's home page
- **get_stores: 171 requests**: Business logic endpoints being accessed
- **unknown: 48 requests**: 404 errors showing error tracking works
- **metrics: 172 requests**: Self-monitoring showing Prometheus scraping

**Why this matters for SRE**: This metric diversity demonstrates that your application is generating production-quality observability data. The presence of both success and error metrics shows your monitoring can track the full user experience.

### Step 3: Focus on Key SRE Metrics

Extract specific metrics relevant to the four golden signals:

```bash
# Check specific application metrics that matter for SRE
curl -s http://$EXTERNAL_IP/metrics | grep -E "(http_requests_total|business_operations)"
```

**Expected output:**
```
# HELP http_requests_total Total number of HTTP requests
# TYPE http_requests_total counter
http_requests_total{endpoint="ready_check",method="GET",status="200"} 1542.0
http_requests_total{endpoint="index",method="GET",status="200"} 84.0
http_requests_total{endpoint="metrics",method="GET",status="200"} 176.0
http_requests_total{endpoint="get_stores",method="GET",status="200"} 168.0
http_requests_total{endpoint="unknown",method="GET",status="404"} 37.0
```

**SRE Analysis of Current Metrics:**
- **Traffic Signal**: Total request volume across all endpoints
- **Error Signal**: 404 responses from unknown endpoints (error rate calculation)
- **Availability Signal**: Successful responses vs. total requests
- **Business Logic**: Store operations showing application-specific functionality

### Step 4: Generate Additional Traffic for Monitoring

Create more diverse traffic patterns to populate your monitoring dashboards:

```bash
# Generate diverse application traffic to create observable patterns
for i in {1..50}; do
  curl -s http://$EXTERNAL_IP/ > /dev/null
  curl -s http://$EXTERNAL_IP/stores > /dev/null
  curl -s http://$EXTERNAL_IP/stores/1 > /dev/null
  curl -s http://$EXTERNAL_IP/health > /dev/null
  sleep 0.1
done
```

**Verify the traffic created observable changes:**

```bash
# Check that metrics have updated
curl -s http://$EXTERNAL_IP/metrics | grep -E "(http_requests_total|business_operations)" | head -10
```

**Expected output showing increases:**
```
http_requests_total{endpoint="ready_check",method="GET",status="200"} 1582.0
http_requests_total{endpoint="index",method="GET",status="200"} 114.0
http_requests_total{endpoint="metrics",method="GET",status="200"} 181.0
http_requests_total{endpoint="get_stores",method="GET",status="200"} 194.0
http_requests_total{endpoint="unknown",method="GET",status="404"} 59.0
```

**Key observations:**
- **index**: Increased by ~30 requests (our loop traffic)
- **get_stores**: Increased by ~26 requests (stores endpoint calls)  
- **unknown**: Increased by ~22 requests (404s from /stores/1)
- **ready_check**: Continues increasing from Kubernetes health checks

**Why this pattern matters**: You've created realistic traffic that demonstrates both successful operations and error conditions, providing the data variety needed for comprehensive monitoring dashboards.

---

## Deploying Prometheus for Production Monitoring

### Step 5: Examine Prometheus Configuration Files

Before deploying, understand the production-ready monitoring infrastructure you're creating:

```bash
# Examine the Prometheus configuration
cat k8s/monitoring/prometheus-config.yaml
```

**Key configuration sections to understand:**

**Service Discovery Configuration:**
```yaml
- job_name: 'sre-demo-app'
  kubernetes_sd_configs:
    - role: pod
  relabel_configs:
    - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
      action: keep
      regex: true
```

**What this configuration enables:**
- **Automatic Discovery**: Finds pods with `prometheus.io/scrape: "true"` annotations
- **Dynamic Scaling**: Automatically includes new pods as they're created
- **Metadata Enrichment**: Adds Kubernetes labels to metrics for better querying

**Examine the RBAC configuration:**

```bash
# Review the RBAC permissions required for service discovery
cat k8s/monitoring/prometheus-rbac.yaml
```

**Understanding RBAC Components:**
- **ServiceAccount**: Dedicated identity for Prometheus pods (`prometheus`)
- **ClusterRole**: Defines read permissions for pods, services, endpoints, and nodes
- **ClusterRoleBinding**: Links the ServiceAccount to the ClusterRole permissions

**Examine the deployment specifications:**

```bash
# Review the production-ready deployment
cat k8s/monitoring/prometheus-deployment.yaml
```

**Production-ready features you'll deploy:**
- **RBAC Permissions**: ServiceAccount with ClusterRole for Kubernetes API access
- **Resource Management**: CPU/memory requests and limits (200m CPU, 512Mi memory requests)
- **Health Checks**: Liveness and readiness probes using Prometheus endpoints
- **External Access**: LoadBalancer service for dashboard and API access
- **Persistent Storage**: EmptyDir volume for metric storage (15-day retention)

---

### Step 6: Deploy Prometheus with RBAC to Your Cluster

Deploy the complete Prometheus monitoring stack with proper Kubernetes service discovery permissions.

**Understanding the deployment components:**

Your Prometheus deployment includes three critical Kubernetes manifests:

1. **prometheus-rbac.yaml**: ServiceAccount, ClusterRole, and ClusterRoleBinding for Kubernetes API access
2. **prometheus-config.yaml**: ConfigMap with scraping configuration and service discovery rules
3. **prometheus-deployment.yaml**: Deployment and Service for the Prometheus server

**Deploy Prometheus in the correct order:**

```bash
# Step 1: Apply RBAC configuration first (required for service discovery)
kubectl apply -f k8s/monitoring/prometheus-rbac.yaml

# Step 2: Apply Prometheus configuration
kubectl apply -f k8s/monitoring/prometheus-config.yaml

# Step 3: Deploy Prometheus server
kubectl apply -f k8s/monitoring/prometheus-deployment.yaml
```

**Expected output:**
```
serviceaccount/prometheus created
clusterrole.rbac.authorization.k8s.io/prometheus created
clusterrolebinding.rbac.authorization.k8s.io/prometheus created
configmap/prometheus-config-with-alertmanager created
configmap/prometheus-alerts created
deployment.apps/prometheus created
service/prometheus-service created
```

**Understanding why RBAC is applied first:**

Prometheus requires permissions to query the Kubernetes API for pod discovery. The RBAC configuration provides:

- **ServiceAccount**: Dedicated identity for Prometheus pods (`prometheus`)
- **ClusterRole**: Read permissions for pods, services, endpoints, and nodes across the cluster
- **ClusterRoleBinding**: Links the ServiceAccount to the ClusterRole permissions

Without these permissions, Prometheus cannot discover your application pods automatically and you'll see errors like:
```
pods is forbidden: User "system:serviceaccount:default:default" cannot list resource "pods"
```

**Verify all resources were created successfully:**

```bash
# Check RBAC resources
kubectl get serviceaccount prometheus
kubectl get clusterrole prometheus
kubectl get clusterrolebinding prometheus

# Check ConfigMaps
kubectl get configmap prometheus-config-with-alertmanager
kubectl get configmap prometheus-alerts

# Check Deployment and Service
kubectl get deployment prometheus
kubectl get service prometheus-service
```

**Expected output:**
```
NAME         SECRETS   AGE
prometheus   0         30s

NAME         CREATED AT
prometheus   2025-10-15T10:30:00Z

NAME         ROLE                     AGE
prometheus   ClusterRole/prometheus   30s

NAME                                    DATA   AGE
prometheus-config-with-alertmanager     1      30s
prometheus-alerts                       1      30s

NAME         READY   UP-TO-DATE   AVAILABLE   AGE
prometheus   1/1     1            1           30s

NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
prometheus-service   LoadBalancer   10.96.123.45    <pending>     9090:31234/TCP   30s
```

**Wait for Prometheus to be fully operational:**

```bash
# Wait for Prometheus deployment to be ready
kubectl wait --for=condition=available --timeout=300s deployment/prometheus
```

**Expected output:**
```
deployment.apps/prometheus condition met
```

**This command waits up to 5 minutes** for Prometheus to reach Available status, including:
- Image pull from Docker Hub
- Configuration mounting from ConfigMaps
- Service discovery initialization
- Health check probes passing

---

### Step 7: Verify Prometheus is Working Correctly

Check that all components are running and properly configured:

```bash
# Check Prometheus service status
kubectl get service prometheus-service
```

**Expected output:**
```
NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)          AGE
prometheus-service   LoadBalancer   10.96.123.45    34.123.45.67    9090:31234/TCP   2m
```

**Understanding the service configuration:**
- **LoadBalancer type**: Provides external internet access to Prometheus
- **External IP**: Your Prometheus web interface address (34.123.45.67 in this example)
- **Port mapping**: 9090 is the standard Prometheus port

**Get your Prometheus access URL:**

```bash
# Get Prometheus external IP for browser access
export PROMETHEUS_IP=$(kubectl get service prometheus-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
echo "Prometheus URL: http://$PROMETHEUS_IP:9090"
```

**Expected output:**
```
Prometheus URL: http://34.123.45.67:9090
```

**Verify Prometheus logs show successful startup:**

```bash
# Check Prometheus pod logs for successful startup
kubectl logs -l app=prometheus --tail=30
```

**Expected healthy startup logs:**
```
ts=2025-10-15T10:30:34.287Z caller=main.go:583 level=info msg="Starting Prometheus Server" mode=server version="(version=2.47.0, branch=HEAD, revision=...)"
ts=2025-10-15T10:30:34.339Z caller=kubernetes.go:329 level=info component="discovery manager scrape" discovery=kubernetes config=sre-demo-app msg="Using pod service account via in-cluster config"
ts=2025-10-15T10:30:34.397Z caller=main.go:1009 level=info msg="Server is ready to receive web requests."
```

**Critical log messages indicating success:**
- **"Starting Prometheus Server"**: Core startup successful
- **"Using pod service account via in-cluster config"**: RBAC authentication working correctly
- **"Server is ready to receive web requests"**: HTTP API operational and ready

**Some warnings are normal:**
```
ts=2025-10-15T10:30:34.452Z caller=klog.go:96 level=warn component=k8s_client_runtime func=Warning msg="v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice"
```
These warnings about deprecated Kubernetes APIs are expected and don't impact functionality.

**Verify NO permission errors appear in logs:**

```bash
# Check for any RBAC permission errors (should return nothing)
kubectl logs -l app=prometheus --tail=50 | grep -i "forbidden"
```

**Expected output:** *No output* (no permission errors)

If you see errors about pods, nodes, or services being "forbidden", the RBAC configuration wasn't applied correctly. Reapply with:
```bash
kubectl apply -f k8s/monitoring/prometheus-rbac.yaml
kubectl rollout restart deployment/prometheus
```

**Access Prometheus web interface:**

Open your browser and navigate to: `http://$PROMETHEUS_IP:9090`

You should see the Prometheus web interface with:
- **Status menu**: Shows server status and configuration
- **Graph tab**: For running PromQL queries
- **Targets page** (Status → Targets): Shows discovered scrape targets

---

## Configuring Google Cloud Monitoring Integration

### Step 8: Enable Required Google Cloud APIs

Enable the essential monitoring APIs for Google Cloud integration:

```bash
# Enable the two essential APIs for monitoring
gcloud services enable monitoring.googleapis.com
gcloud services enable logging.googleapis.com
```

**Both commands should complete successfully** without errors. These APIs provide the foundation for Google Cloud Monitoring integration.

**Verify the essential APIs were enabled successfully:**

```bash
# Verify the critical APIs are working
gcloud services list --enabled --filter="name:(monitoring.googleapis.com OR logging.googleapis.com)"
```

**Expected successful output:**
```
NAME                       TITLE
logging.googleapis.com     Cloud Logging API
monitoring.googleapis.com  Cloud Monitoring API
```

**Why this matters for SRE work**: In production environments, API permissions are often restricted by security policies. Learning to work with partial permissions and identify which components are essential vs. optional is a critical SRE skill.

### Step 9: Attempt Google Managed Prometheus Integration

Try to configure advanced Google Cloud integration while handling expected failures:

```bash
# Apply Google Managed Prometheus configuration
kubectl apply -f k8s/monitoring/gmp-config.yaml
```

**Expected output with partial success:**
```
configmap/gmp-config unchanged
error: resource mapping not found for name: "sre-demo-app-monitor" namespace: "" from "k8s/monitoring/gmp-config.yaml": no matches for kind "PodMonitor" in version "monitoring.coreos.com/v1"
ensure CRDs are installed first
```

**Understanding this expected error:**
- **ConfigMap created successfully**: Basic Google Cloud configuration applied
- **PodMonitor error**: Custom Resource Definition (CRD) not installed
- **This is expected**: Not all GKE clusters have Prometheus Operator CRDs

**What this teaches about production monitoring**:
- Self-hosted Prometheus (which you deployed) works reliably across environments
- Advanced integrations may have dependencies that aren't always available
- Monitoring strategies must be resilient to partial integration failures

### Step 10: Verify Google Cloud Monitoring Access

Check your Google Cloud Monitoring status:

```bash
# Get your project monitoring URL
export PROJECT_ID=$(gcloud config get-value project)
echo "Google Cloud Monitoring URL:"
echo "https://console.cloud.google.com/monitoring/overview?project=$PROJECT_ID"
```

**Expected output:**
```
Google Cloud Monitoring URL:
https://console.cloud.google.com/monitoring/overview?project=gcp-sre-lab
```

**Check for custom metrics (expected to be empty initially):**

```bash
# List available log-based metrics (will be empty initially)
gcloud logging metrics list --limit=10
```

**Expected output:**
```
Listed 0 items.
```

**This is completely normal.** Custom log-based metrics take time to populate, and your primary monitoring is through self-hosted Prometheus, which provides immediate results.

---

## Real-Time Monitoring with Multiple Terminals

### Step 11: Set Up Coordinated Monitoring Sessions

This section teaches you to observe metrics in real-time, a critical skill for SRE incident response and system understanding.

**Terminal Setup Strategy:**
- **Terminal 1**: Prometheus queries and monitoring
- **Terminal 2**: Traffic generation and application testing
- **Terminal 3**: Kubernetes monitoring and log observation

**Open multiple terminal sessions** for this coordinated monitoring exercise.

**In Terminal 1 - Set up your monitoring environment:**

```bash
# Terminal 1: Set up monitoring variables
cd exercises/exercise4
export PROMETHEUS_IP=$(kubectl get service prometheus-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
export EXTERNAL_IP=$(kubectl get service sre-demo-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

echo "Monitoring URLs:"
echo "Prometheus: http://$PROMETHEUS_IP:9090"
echo "Application: http://$EXTERNAL_IP"
```

**In Terminal 2 - Prepare for traffic generation:**

```bash
# Terminal 2: Set up traffic generation
cd exercises/exercise4
export EXTERNAL_IP=$(kubectl get service sre-demo-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
echo "Ready to generate traffic to: http://$EXTERNAL_IP"
```

### Step 12: Generate Traffic While Monitoring

**In Terminal 2 - Start generating realistic traffic patterns:**

```bash
# Generate continuous realistic traffic
for i in {1..100}; do
  curl -s http://$EXTERNAL_IP/ > /dev/null &
  curl -s http://$EXTERNAL_IP/stores > /dev/null &
  curl -s http://$EXTERNAL_IP/health > /dev/null &
  
  if [ $((i % 10)) -eq 0 ]; then
    echo "Generated $i iterations ($(($i * 3)) total requests)..."
  fi
  
  sleep 1
done
```

**Expected output in Terminal 2:**
```
Generated 10 iterations (30 total requests)...
Generated 20 iterations (60 total requests)...
Generated 30 iterations (90 total requests)...
```

**In Terminal 1 - Monitor metrics changes in real-time:**

```bash
# Watch metrics change in real-time (run this while traffic is generating)
watch -n 5 "curl -s http://$EXTERNAL_IP/metrics | grep 'http_requests_total{endpoint=\"index\"' | tail -3"
```

**What you should observe**: The counter values increasing every 5 seconds as your traffic generation creates new requests.

**In your browser - Open Prometheus Web UI:**

1. **Navigate to**: `http://$PROMETHEUS_IP:9090`
2. **Go to Graph tab**
3. **Run this query**: `sum(rate(http_requests_total[1m]))`
4. **Click Execute and switch to Graph view**

You should see the request rate spike corresponding to your traffic generation.

### Step 13: Test the SRE Query Reference

Use the comprehensive query reference to understand your application's behavior:

```bash
# Examine the SRE query reference
cat monitoring/sre-queries.md
```

**Test these key queries in your Prometheus web interface** (`http://$PROMETHEUS_IP:9090`):

**Essential SRE Queries to Test:**

1. **Request Rate**: `sum(rate(http_requests_total[5m]))`
2. **Error Rate**: `sum(rate(http_requests_total{status_code=~"5.."}[5m])) / sum(rate(http_requests_total[5m])) * 100`
3. **P95 Latency**: `histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))`
4. **Endpoint Breakdown**: `sum(rate(http_requests_total[5m])) by (endpoint)`

**Expected results when running queries during traffic generation:**
- **Request Rate**: Should show values > 0 (requests per second)
- **Error Rate**: Should show percentage of 404 errors from /stores/1 requests
- **P95 Latency**: Should show response time in seconds
- **Endpoint Breakdown**: Should show traffic distribution across endpoints

---

## Understanding Prometheus Service Discovery Issues

### Step 14: Diagnose Service Discovery Configuration

This step addresses a common production issue you may encounter: service discovery not finding your application targets.

**Check if Prometheus is discovering your application:**

**In your browser, go to Prometheus UI** (`http://$PROMETHEUS_IP:9090`):
1. **Click "Status" menu → "Targets"**
2. **Look for entries with job "sre-demo-app"**

**If you see "No targets found" or your application isn't listed**, this is a common configuration issue in production environments.

**Diagnose the service discovery issue:**

```bash
# Check if your SRE application pods have the correct annotations
kubectl get pods -l app=sre-demo-app -o yaml | grep -A 5 -B 5 "prometheus.io"
```

**If you see no prometheus.io annotations**, this explains why service discovery isn't working.

**Check your deployment configuration:**

```bash
# Look at your current deployment annotations
kubectl get deployment sre-demo-app -o yaml | grep -A 10 -B 10 "annotations"
```

**Understanding the issue**: Your deployment from Exercise 3 may not have included the Prometheus service discovery annotations that Prometheus needs to automatically find and scrape your application.

### Step 15: Fix Service Discovery (If Needed)

**If service discovery isn't working, add the required annotations:**

```bash
# Add Prometheus scraping annotations to your deployment
kubectl patch deployment sre-demo-app -p '{
  "spec": {
    "template": {
      "metadata": {
        "annotations": {
          "prometheus.io/scrape": "true",
          "prometheus.io/port": "80",
          "prometheus.io/path": "/metrics"
        }
      }
    }
  }
}'
```

**Expected output:**
```
deployment.apps/sre-demo-app patched
```

**Wait for the deployment to update:**

```bash
# Wait for rollout to complete
kubectl rollout status deployment/sre-demo-app
```

**Verify the annotations were added:**

```bash
# Check that pods now have the required annotations
kubectl get pods -l app=sre-demo-app -o yaml | grep -A 3 "prometheus.io"
```

**Expected output:**
```
prometheus.io/path: /metrics
prometheus.io/port: "80"
prometheus.io/scrape: "true"
```

**Give Prometheus time to discover the new targets** (this can take 1-2 minutes).

### Step 16: Verify Service Discovery Success

**Check Prometheus targets again** in your browser:
1. **Refresh the Targets page** in Prometheus UI
2. **Look for your sre-demo-app entries**
3. **They should show "UP" status**

**Test that Prometheus is now collecting metrics:**

**In Prometheus UI, try these queries again:**
- `sum(rate(http_requests_total[5m]))`
- `up{job="sre-demo-app"}`

**If queries now return data**, service discovery is working correctly.

**If you're still not seeing data**: This is normal in some educational environments. The important learning is understanding the service discovery process and troubleshooting steps.

---

## Creating and Testing SRE Queries

### Step 17: Build SRE-Focused Monitoring Queries

Whether or not service discovery is fully working, you can still learn to build production SRE queries by testing them against your application directly.

**Test queries that SRE teams use for production monitoring:**

**In Terminal 1 - Test queries directly against your application:**

```bash
# Get current metrics for SRE analysis
curl -s http://$EXTERNAL_IP/metrics > current_metrics.txt

# Analyze request distribution
echo "=== Request Distribution by Endpoint ==="
grep "http_requests_total" current_metrics.txt | grep -v "# " | sort
```

**Understanding SRE Query Patterns:**

**Golden Signal 1 - Latency Analysis:**
```bash
# Extract latency histogram data
echo "=== P95 Latency Analysis ==="
grep "http_request_duration_seconds_bucket.*le=\"0.1\"" current_metrics.txt
```

**Golden Signal 2 - Traffic Analysis:**
```bash
# Calculate total traffic volume
echo "=== Traffic Volume Analysis ==="
grep "http_requests_total" current_metrics.txt | grep -v "# " | \
  awk -F'{' '{print $2}' | awk -F'}' '{print $1}' | \
  awk -F' ' '{sum += $NF} END {print "Total requests:", sum}'
```

**Golden Signal 3 - Error Analysis:**
```bash
# Identify error patterns
echo "=== Error Rate Analysis ==="
grep "status.*404" current_metrics.txt | \
  awk -F' ' '{sum += $NF} END {print "Total 404 errors:", sum}'
```

### Step 18: Create a Google Cloud Monitoring Dashboard

Attempt to create a dashboard that demonstrates enterprise monitoring practices:

```bash
# Examine the dashboard configuration
cat monitoring/dashboard-config.json
```

**Create the dashboard:**

```bash
# Create dashboard (may encounter configuration errors)
gcloud monitoring dashboards create --config-from-file=monitoring/dashboard-config.json
```

**If you see an error like:**
```
ERROR: (gcloud.monitoring.dashboards.create) INVALID_ARGUMENT: Field mosaicLayout.columns has an invalid value of "0": must be in the range (1,48).
```

**This is expected in some environments.** The important learning is understanding dashboard-as-code concepts and the metrics queries used in production monitoring.

**Access Google Cloud Monitoring anyway:**

```bash
# Get your monitoring dashboard URL
echo "Access Google Cloud Monitoring at:"
echo "https://console.cloud.google.com/monitoring/overview?project=$PROJECT_ID"
```

**Even without custom dashboards**, Google Cloud Monitoring provides:
- **Infrastructure metrics**: CPU, memory, network for your GKE cluster
- **Kubernetes metrics**: Pod health, service status
- **GKE-specific monitoring**: Node health, cluster resource utilization

---

## Final Objective

By completing this exercise, you have successfully demonstrated:

Your ability to deploy a production-ready Prometheus monitoring stack in Kubernetes with proper RBAC configuration, your understanding of Kubernetes RBAC principles and how they secure cluster resources, your skills in real-time traffic coordination and metric observation across multiple terminals, your knowledge of the four golden signals and how to query them using PromQL, your experience with dashboard-as-code concepts and production monitoring design principles, and your understanding of how monitoring architectures must adapt to different cluster configurations and permission constraints.

### Verification Questions

Test your understanding by answering these questions:

1. **Why is** RBAC configuration required for Prometheus service discovery, and what specific permissions does it need?
2. **Why might** Prometheus service discovery fail to find your application pods, and what specific annotations are required?
3. **How do** advanced monitoring integrations like PodMonitor differ from basic Prometheus configuration?
4. **What are** the four golden signals, and which PromQL queries would you use to measure each one?
5. **How would** you design a dashboard that supports both technical monitoring and business decision-making?
6. **Why is** understanding configuration limitations as important as successful deployments?

### Key Monitoring Insights Gained

**Production Monitoring Complexity**: Enterprise monitoring systems require multiple layers of configuration, from basic metric collection to advanced service discovery and dashboard automation. Understanding both simple and complex approaches prepares you for various production environments.

**RBAC is Essential**: Kubernetes RBAC is not optional for production deployments. Prometheus requires specific permissions to discover pods, and understanding how to configure ServiceAccounts, ClusterRoles, and ClusterRoleBindings is critical for secure operations.

**Configuration Resilience**: Monitoring strategies must work across different cluster configurations, permission levels, and integration capabilities. Building monitoring that degrades gracefully when advanced features aren't available ensures reliable observability.

**Real-Time Coordination Skills**: Effective SRE work requires coordinating traffic generation, metric observation, and system analysis across multiple tools simultaneously. This coordination becomes critical during incident response and system optimization efforts.

**Dashboard Design Principles**: Production dashboards must balance technical detail with business context, providing both immediate operational insights and longer-term trend analysis. Understanding dashboard-as-code approaches enables consistent monitoring experiences across teams.

---

## Troubleshooting Common Issues

### Service Discovery Problems

**Prometheus not discovering application targets**

**Symptoms:**
- Prometheus Targets page shows no entries for `sre-demo-app`
- Service discovery appears to not be working

**Solution:**
Verify that your application pods have the correct `prometheus.io/scrape: "true"` annotation:
```bash
kubectl get pods -l app=sre-demo-app -o yaml | grep prometheus.io
```

If annotations are missing, patch your deployment:
```bash
kubectl patch deployment sre-demo-app -p '{
  "spec": {
    "template": {
      "metadata": {
        "annotations": {
          "prometheus.io/scrape": "true",
          "prometheus.io/port": "80",
          "prometheus.io/path": "/metrics"
        }
      }
    }
  }
}'
```

**Prometheus showing permission errors**

**Symptoms:**
- Logs contain: `pods is forbidden: User "system:serviceaccount:default:default" cannot list resource "pods"`
- Prometheus cannot discover any Kubernetes resources

**Root Cause:**
RBAC configuration not applied or Prometheus deployment using wrong ServiceAccount.

**Solution:**
```bash
# Verify RBAC resources exist
kubectl get serviceaccount prometheus
kubectl get clusterrole prometheus
kubectl get clusterrolebinding prometheus

# If missing, reapply RBAC
kubectl apply -f k8s/monitoring/prometheus-rbac.yaml

# Verify Prometheus is using correct ServiceAccount
kubectl get deployment prometheus -o jsonpath='{.spec.template.spec.serviceAccountName}'
# Should output: prometheus

# If incorrect, reapply deployment
kubectl apply -f k8s/monitoring/prometheus-deployment.yaml

# Restart Prometheus
kubectl rollout restart deployment/prometheus
kubectl rollout status deployment/prometheus

# Verify logs are clean
kubectl logs -l app=prometheus --tail=30 | grep -i forbidden
# Should return nothing
```

**Targets showing "DOWN" status**

**Symptoms:**
- Prometheus discovers targets but they show as "DOWN"

**Solution:**
Check network connectivity between Prometheus and application pods:
```bash
# Get application pod IP
kubectl get pods -l app=sre-demo-app -o wide

# Test connectivity from Prometheus pod
kubectl exec -it $(kubectl get pod -l app=prometheus -o name) -- \
  wget -O- http://<app-pod-ip>:80/metrics
```

Verify that the application is actually exposing metrics on the specified port and path, and confirm that any network policies allow traffic between the monitoring and application namespaces.

**Metrics appear and disappear intermittently**

This often indicates pod restarts due to resource limits or health check failures. Monitor pod stability:
```bash
kubectl get pods -w
```

Check if Prometheus resource limits need adjustment.

### Query and Data Issues

**PromQL queries return no data**: Verify that your time range covers periods when your application was receiving traffic, check that metric names match exactly (Prometheus is case-sensitive), and confirm that rate queries use appropriate time windows (5m minimum for meaningful rate calculations).

**High latency in metric collection**: Check if your Prometheus instance is resource-constrained with `kubectl top pod`, verify that scrape intervals aren't too aggressive for your application's capacity, and consider if high cardinality labels are causing memory pressure.

**Google Cloud Monitoring integration missing data**: Verify that required APIs are enabled and accessible, understand that custom metrics can take 5-10 minutes to appear, and check that your GKE cluster has appropriate IAM permissions for Cloud Monitoring.

### Permission and API Issues

**Cloud API permission errors**: These are often expected in educational environments and don't prevent core functionality. Focus on the monitoring capabilities that work (self-hosted Prometheus) and understand which cloud features require additional permissions.

**RBAC permission denied errors**: Ensure that the Prometheus ServiceAccount has the correct ClusterRole bindings for Kubernetes API access, verify that your GKE cluster has RBAC enabled, and check if workspace-specific permissions are required.

**LoadBalancer IP stuck in pending**: Verify that your Google Cloud project has available external IP quota, check that the required APIs are enabled, and ensure that your GKE cluster has proper network configuration.

---

## Next Steps

You have successfully implemented comprehensive monitoring capabilities with proper RBAC configuration and service discovery. You've learned how Kubernetes security integrates with observability, how to troubleshoot common permission issues, and how to create queries that support SRE decision-making.

**Proceed to [Exercise 5](../exercise5/)** where you will implement intelligent alerting and incident response workflows based on the monitoring infrastructure established in this exercise, define Service Level Indicators (SLIs) and Service Level Objectives (SLOs) that measure user experience, create alert policies that signal actionable problems without generating noise, and establish incident response procedures that support effective problem resolution and minimize Mean Time to Resolution (MTTR).

**Key Concepts to Remember**: Effective monitoring provides visibility into system behavior but requires intelligent alerting to drive action. RBAC configuration is essential for secure Kubernetes operations and enables Prometheus to discover targets dynamically. Service discovery and metric collection are the foundation, but alert policies and incident response procedures determine operational effectiveness. Self-hosted Prometheus provides reliable metrics collection across diverse environments, while structured observability data enables both automated alerting and manual investigation during incidents.

**Before Moving On**: Ensure you can explain:
- How Kubernetes RBAC enables Prometheus service discovery
- Why ServiceAccounts, ClusterRoles, and ClusterRoleBindings are necessary
- How your monitoring stack collects and stores metrics
- Why service discovery configuration is critical for dynamic environments
- How the metrics you've implemented support both immediate troubleshooting and long-term reliability analysis

In the next exercise, you'll build alerting policies that convert this observability data into actionable incident response workflows.